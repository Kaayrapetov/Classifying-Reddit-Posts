{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55cc250",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\" />\n",
    "\n",
    "# Project 3: NLP Analysis of Reddit Data \n",
    "## Part 2: Data Cleaning\n",
    "\n",
    "Katrin Ayrapetov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f57ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553d3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_elem_df =  pd.read_csv('comments_elem_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef7ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_elem_df =  pd.read_csv('submissions_elem_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb6b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_teach_ms =  pd.read_csv('submissions_teach_ms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2235c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs =  pd.read_csv('df_hs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3826fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the comments and submissions data sets \n",
    "df_elem = pd.concat([comments_elem_df, submissions_elem_df], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eaa9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with comments that have been deleted by the moderator or removed by the user. \n",
    "df_elem.drop(df_elem[df_elem['body'] == \"[deleted]\"].index, inplace = True)\n",
    "df_elem.drop(df_elem[df_elem['body'] == \"[removed]\"].index, inplace = True)\n",
    "df_hs.drop(df_hs[df_hs['body'] == \"[removed]\"].index, inplace = True)\n",
    "df_hs.drop(df_hs[df_hs['body'] == \"[deleted]\"].index, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa28f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index \n",
    "df_elem.reset_index(drop=\"True\",inplace=True)\n",
    "df_hs.reset_index(drop=\"True\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "728bdad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates and keep the first \n",
    "df_elem.drop_duplicates(keep = \"first\", inplace = True)\n",
    "df_hs.drop_duplicates(keep = \"first\", inplace = True)\n",
    "#Reset the index after dropping the duplicates \n",
    "df_elem.reset_index(drop=\"True\",inplace=True)\n",
    "df_hs.reset_index(drop=\"True\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e6997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrrite the string with any internet address that begins in \"http\" or ends with \".com\" removed.\n",
    "#for the elementary school data set. \n",
    "\n",
    "for i in range(df_elem.shape[0]):\n",
    "    re.sub(r'http\\S+', '', df_elem.iloc[i]['body'].lower())\n",
    "    re.sub(r'\\S+.com', '', df_elem.iloc[i]['body'].lower())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d141346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrrite the string with any internet address that begins in \"http\" or ends with \".com\" removed\n",
    "#for the high school data set. \n",
    "\n",
    "for i in range(df_hs.shape[0]):\n",
    "    re.sub(r'http\\S+', '', df_hs.iloc[i]['body'].lower())\n",
    "    re.sub(r'\\S+.com', '', df_hs.iloc[i]['body'].lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4576e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of accent marks, umlats or any other above letter symbols.  \n",
    "import unicodedata\n",
    "df_elem['body'] = df_elem['body'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df_hs['body'] = df_hs['body'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1ea1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Expand contracted words in the elementary teachers data set, so that \"can't\" will read \"cannot\" #### \n",
    "import contractions \n",
    "\n",
    "for i in range(df_elem.shape[0]):\n",
    "    expanded_words = []\n",
    "    for word in df_elem.iloc[i]['body'].split():\n",
    "        expanded_words.append(contractions.fix(word))\n",
    "        df_elem.iloc[i]['body'] = ' '.join(expanded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec48751",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Expand contracted words in the high school data set, so that \"can't\" will read \"cannot\" #### \n",
    "import contractions \n",
    "\n",
    "for i in range(df_hs.shape[0]):\n",
    "    expanded_words = []\n",
    "    for word in df_hs.iloc[i]['body'].split():\n",
    "        expanded_words.append(contractions.fix(word))\n",
    "        df_hs.iloc[i]['body'] = ' '.join(expanded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c76bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty column to store cleaned comments for elementary data set. \n",
    "df_elem['CLEAN_COMMENT'] = df_elem.apply(lambda _: '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "825a9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty column to store cleaned comments for middle school data set. \n",
    "df_hs['CLEAN_COMMENT'] = df_hs.apply(lambda _: '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3f0680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list called \"cheat list\" that includes \"stop words\" particular to our problem \n",
    "cheat_words = [\"elementary\", \"Elementary\",\"Elementary\",\"middle\",\"high\",\"High\",\"school\",\"School\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "639739eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Createa a function that takes in a dataframe, the name of the column where strings are stored, \n",
    "# and the name of the column where cleaned string will be stored. The function clean_comments does \n",
    "# the following: \n",
    "# (1) Removes special symbols and numbers\n",
    "# (2) Removes stop words\n",
    "# (3) Removes words in \"cheat list\"\n",
    "# (3) Changes the words to the lemmatized version \n",
    "# (4) Creates a new clean string \n",
    "\n",
    "\n",
    "def clean_comments(df,column_name,new_column_name):\n",
    "    dim = df.shape[0]\n",
    "    for i in range(dim):\n",
    "        # Remove the punctuation .\n",
    "        # This creates a list called \"words_sans_punctuation\" which contains all the words in the status\n",
    "        #with punctuations,numbers, and special symbols removed.\n",
    "        words_sans_punctuation = re.findall(r'[a-z]*',df.iloc[i][column_name].lower())\n",
    "        # This creates a list called \"words_sans_stop_words\" which contains all the words in the status\n",
    "        #with punctuations,numbers, and special symbols removed. \n",
    "        words_sans_cheat_words = [w for w in list(filter((\"\").__ne__, words_sans_punctuation)) if not w in cheat_words]\n",
    "        words_sans_stop_words = [w for w in list(filter((\"\").__ne__, words_sans_cheat_words)) if not w in stopwords.words('english')]\n",
    "        \n",
    "        # Replace the words with their lemmatized version. \n",
    "        # This creates a list called \"lemms_of_words\" which contains the lemmatized version\n",
    "        # of all the words in the facebook status. \n",
    "        lemms_of_words = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for w in list(filter((\"\").__ne__, words_sans_stop_words)):\n",
    "            lemms_of_words.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "        from spellchecker import SpellChecker\n",
    "        spell = SpellChecker()\n",
    "        words_spell_checked = [spell.correction(word) for word in list(filter((\"\").__ne__, lemms_of_words))]\n",
    "        # Creates a new clean string \n",
    "        cleaned_status  = \" \".join(list(filter((\"\").__ne__, words_spell_checked)))\n",
    "        # Stores the new clean string into a new column \n",
    " \n",
    "        df.at[i, new_column_name] = cleaned_status\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e0efb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comments(df_elem,'body','CLEAN_COMMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0296b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comments(df_hs,'body','CLEAN_COMMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a403361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the clean dataframe to Excel SpreadSheets \n",
    "df_elem.to_csv('df_elem_CLEAN.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef7da288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs.to_csv('df_hs_CLEAN.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c42c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elem_clean =  pd.read_csv('df_elem_CLEAN.csv')\n",
    "\n",
    "df_hs_clean =  pd.read_csv('df_hs_CLEAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63acde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract only the adjectives and the adverbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54bb76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_elem_hs = pd.concat([df_elem, df_hs], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f5692a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Drop the null values. \n",
    "df_clean_elem_hs.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=True)\n",
    "#Reset the index \n",
    "df_clean_elem_hs.reset_index(drop=\"True\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f282cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5519, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_elem_hs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58dd2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty column to store only adjectives and adverbs of the data for middle school data set. \n",
    "df_clean_elem_ms['ADJ_ADV_CLEAN_COMMENT'] = df_clean_elem_ms.apply(lambda _: '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a5cd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty column to store only adjectives and adverbs of the data for middle school data set. \n",
    "df_clean_elem_hs['ADJ_ADV_CLEAN_COMMENT'] = df_clean_elem_hs.apply(lambda _: '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54ec9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_adv_extractor(df,column_name,new_column_name):\n",
    "    dim = df.shape[0]\n",
    "    from nltk import pos_tag\n",
    "    for i in range(dim):\n",
    "        words_with_pos = pos_tag(df.iloc[i][column_name].split())\n",
    "        list_of_parts_of_speech = [\"JJ\",\"JJR\",\"JJS\"]\n",
    "        adverbs_and_adjectives = []\n",
    "        for pair in words_with_pos:\n",
    "            if pair[1] in list_of_parts_of_speech:\n",
    "                adverbs_and_adjectives.append(pair[0])\n",
    "        \n",
    "            adj_clean_text  = \" \".join(list(filter((\"\").__ne__, adverbs_and_adjectives)))\n",
    "            df.at[i, new_column_name] = adj_clean_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd6e8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_adv_extractor(df_clean_elem_ms,'CLEAN_COMMENT','ADJ_ADV_CLEAN_COMMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "596ba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Drop the null values. \n",
    "df_clean_elem_hs.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=True)\n",
    "#Reset the index \n",
    "df_clean_elem_hs.reset_index(drop=\"True\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "836a5201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>CLEAN_COMMENT</th>\n",
       "      <th>ADJ_ADV_CLEAN_COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElementaryTeachers</td>\n",
       "      <td>Representation is important in the classroom. ...</td>\n",
       "      <td>representation important classroom every child...</td>\n",
       "      <td>important least</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElementaryTeachers</td>\n",
       "      <td>anything to keep math from being too dry (and ...</td>\n",
       "      <td>anything keep math dry intimidating student ab...</td>\n",
       "      <td>keep dry much great able different different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElementaryTeachers</td>\n",
       "      <td>-Sometimes redirection is just the name of the...</td>\n",
       "      <td>sometimes redirection name game honestly say r...</td>\n",
       "      <td>good general copy teacher sure table fun kid y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElementaryTeachers</td>\n",
       "      <td>If it feels like it should not be happening, i...</td>\n",
       "      <td>feel like happening probably happening sort be...</td>\n",
       "      <td>arm sort common positive good respectful respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElementaryTeachers</td>\n",
       "      <td>We all die soon. May as well make this life wo...</td>\n",
       "      <td>die soon may well make life worth positive eve...</td>\n",
       "      <td>positive everyday pal positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                               body  \\\n",
       "0  ElementaryTeachers  Representation is important in the classroom. ...   \n",
       "1  ElementaryTeachers  anything to keep math from being too dry (and ...   \n",
       "2  ElementaryTeachers  -Sometimes redirection is just the name of the...   \n",
       "3  ElementaryTeachers  If it feels like it should not be happening, i...   \n",
       "4  ElementaryTeachers  We all die soon. May as well make this life wo...   \n",
       "\n",
       "                                       CLEAN_COMMENT  \\\n",
       "0  representation important classroom every child...   \n",
       "1  anything keep math dry intimidating student ab...   \n",
       "2  sometimes redirection name game honestly say r...   \n",
       "3  feel like happening probably happening sort be...   \n",
       "4  die soon may well make life worth positive eve...   \n",
       "\n",
       "                               ADJ_ADV_CLEAN_COMMENT  \n",
       "0                                    important least  \n",
       "1       keep dry much great able different different  \n",
       "2  good general copy teacher sure table fun kid y...  \n",
       "3  arm sort common positive good respectful respo...  \n",
       "4                     positive everyday pal positive  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_elem_hs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c5bef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_elem_hs.to_csv('df_clean_elem_hs.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ac4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
